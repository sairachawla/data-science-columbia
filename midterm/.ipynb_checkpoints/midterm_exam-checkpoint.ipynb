{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "626117bf",
   "metadata": {},
   "source": [
    "# Midterm - Saira Chawla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ec50f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries & functionsimport pandas as pd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0019cd24",
   "metadata": {},
   "source": [
    "1.  Import the spam dataset and print the first six rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f3b6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make:</th>\n",
       "      <th>word_freq_address:</th>\n",
       "      <th>word_freq_all:</th>\n",
       "      <th>word_freq_3d:</th>\n",
       "      <th>word_freq_our:</th>\n",
       "      <th>word_freq_over:</th>\n",
       "      <th>word_freq_remove:</th>\n",
       "      <th>word_freq_internet:</th>\n",
       "      <th>word_freq_order:</th>\n",
       "      <th>word_freq_mail:</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;:</th>\n",
       "      <th>char_freq_(:</th>\n",
       "      <th>char_freq_[:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "      <th>char_freq_$:</th>\n",
       "      <th>char_freq_#:</th>\n",
       "      <th>capital_run_length_average:</th>\n",
       "      <th>capital_run_length_longest:</th>\n",
       "      <th>capital_run_length_total:</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make:  word_freq_address:  word_freq_all:  word_freq_3d:  \\\n",
       "0             0.00                0.64            0.64            0.0   \n",
       "1             0.21                0.28            0.50            0.0   \n",
       "2             0.06                0.00            0.71            0.0   \n",
       "3             0.00                0.00            0.00            0.0   \n",
       "4             0.00                0.00            0.00            0.0   \n",
       "5             0.00                0.00            0.00            0.0   \n",
       "\n",
       "   word_freq_our:  word_freq_over:  word_freq_remove:  word_freq_internet:  \\\n",
       "0            0.32             0.00               0.00                 0.00   \n",
       "1            0.14             0.28               0.21                 0.07   \n",
       "2            1.23             0.19               0.19                 0.12   \n",
       "3            0.63             0.00               0.31                 0.63   \n",
       "4            0.63             0.00               0.31                 0.63   \n",
       "5            1.85             0.00               0.00                 1.85   \n",
       "\n",
       "   word_freq_order:  word_freq_mail:  ...  char_freq_;:  char_freq_(:  \\\n",
       "0              0.00             0.00  ...          0.00         0.000   \n",
       "1              0.00             0.94  ...          0.00         0.132   \n",
       "2              0.64             0.25  ...          0.01         0.143   \n",
       "3              0.31             0.63  ...          0.00         0.137   \n",
       "4              0.31             0.63  ...          0.00         0.135   \n",
       "5              0.00             0.00  ...          0.00         0.223   \n",
       "\n",
       "   char_freq_[:  char_freq_!:  char_freq_$:  char_freq_#:  \\\n",
       "0           0.0         0.778         0.000         0.000   \n",
       "1           0.0         0.372         0.180         0.048   \n",
       "2           0.0         0.276         0.184         0.010   \n",
       "3           0.0         0.137         0.000         0.000   \n",
       "4           0.0         0.135         0.000         0.000   \n",
       "5           0.0         0.000         0.000         0.000   \n",
       "\n",
       "   capital_run_length_average:  capital_run_length_longest:  \\\n",
       "0                        3.756                           61   \n",
       "1                        5.114                          101   \n",
       "2                        9.821                          485   \n",
       "3                        3.537                           40   \n",
       "4                        3.537                           40   \n",
       "5                        3.000                           15   \n",
       "\n",
       "   capital_run_length_total:  spam  \n",
       "0                        278     1  \n",
       "1                       1028     1  \n",
       "2                       2259     1  \n",
       "3                        191     1  \n",
       "4                        191     1  \n",
       "5                         54     1  \n",
       "\n",
       "[6 rows x 58 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam = pd.read_csv(\"spam_dataset.csv\")\n",
    "spam.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3255a67",
   "metadata": {},
   "source": [
    "2.  Read through the documentation of the original dataset here:\n",
    "\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names\n",
    "\n",
    "The dependent variable is \"spam\" where one indicates that an email is spam and zero otherwise.  Which three variables in the dataset do you think will be important predictors in a model of spam?  Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04485680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['word_freq_make:', 'word_freq_address:', 'word_freq_all:',\n",
       "       'word_freq_3d:', 'word_freq_our:', 'word_freq_over:',\n",
       "       'word_freq_remove:', 'word_freq_internet:', 'word_freq_order:',\n",
       "       'word_freq_mail:', 'word_freq_receive:', 'word_freq_will:',\n",
       "       'word_freq_people:', 'word_freq_report:', 'word_freq_addresses:',\n",
       "       'word_freq_free:', 'word_freq_business:', 'word_freq_email:',\n",
       "       'word_freq_you:', 'word_freq_credit:', 'word_freq_your:',\n",
       "       'word_freq_font:', 'word_freq_000:', 'word_freq_money:',\n",
       "       'word_freq_hp:', 'word_freq_hpl:', 'word_freq_george:',\n",
       "       'word_freq_650:', 'word_freq_lab:', 'word_freq_labs:',\n",
       "       'word_freq_telnet:', 'word_freq_857:', 'word_freq_data:',\n",
       "       'word_freq_415:', 'word_freq_85:', 'word_freq_technology:',\n",
       "       'word_freq_1999:', 'word_freq_parts:', 'word_freq_pm:',\n",
       "       'word_freq_direct:', 'word_freq_cs:', 'word_freq_meeting:',\n",
       "       'word_freq_original:', 'word_freq_project:', 'word_freq_re:',\n",
       "       'word_freq_edu:', 'word_freq_table:', 'word_freq_conference:',\n",
       "       'char_freq_;:', 'char_freq_(:', 'char_freq_[:', 'char_freq_!:',\n",
       "       'char_freq_$:', 'char_freq_#:', 'capital_run_length_average:',\n",
       "       'capital_run_length_longest:', 'capital_run_length_total:', 'spam'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8538e4d",
   "metadata": {},
   "source": [
    "The three variables I think will be important are capital_run_length_average:, char_freq_!:, and word_freq_free: because the first two variables represent a type of email formatting (capital letters, !) that is considered unprofessional and many spam emails contain incentives with the word free. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb97840",
   "metadata": {},
   "source": [
    "3.  Visualize the univariate distribution of each of the variables in the previous question.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35c18ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spam['capital_run_length_average:'].hist(log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a90a060b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spam['char_freq_!:'].hist(log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46efa77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spam['word_freq_free:'].hist(log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43591e78",
   "metadata": {},
   "source": [
    "4. Name each of the supervised learning models that we have learned thus far that are used to predict dependent variables like \"spam\".   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a6dfb3",
   "metadata": {},
   "source": [
    "So far, the models that are used to predict categorical variables such as \"spam\" are k-nearest neighbors, logistic regression, support vector classifiers, and decision trees because all of them can be used for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc8cbdd",
   "metadata": {},
   "source": [
    "5. Describe the importance of training and test data.  Why do we separate data into these subsets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3653b267",
   "metadata": {},
   "source": [
    "The importance of separating data into a training and a test dataframe lies in the ability to build and test our model. Regardless of what model, it will train on a training dataset and be tested on the test dataset. (Hence, the names.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883746d8",
   "metadata": {},
   "source": [
    "6. What is k-fold cross validation and what do we use it for?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4cc3bb",
   "metadata": {},
   "source": [
    "Cross validation allows us to evaluate the prediction error of models. K-fold cross validation trains a number of models on different sets of input data specified by k. K-fold cross validation is used to build a more generalized model and prevent overfitting. \n",
    "\n",
    "Here is the process of k-fold cross validation:\n",
    "- Fold 1 used as test for model 1; folds 2-5 used to train model 1\n",
    "- Fold 1 used as test for model 2; folds 1, 3-5 used to train model 2\n",
    "- continues on... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469c1567",
   "metadata": {},
   "source": [
    "7. How is k-fold cross validation different from stratified k-fold cross validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6bb8e3",
   "metadata": {},
   "source": [
    "Stratified ensures that each fold of the data will have the same proportion of observations of a given label whereas standard k-fold does not take proportion of classes into account. This results in a more reliable and generalized model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77323278",
   "metadata": {},
   "source": [
    "8. Choose one model from question four.  Split the data into training and test subsets.  Build a model with the three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k).  Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2d7fd4",
   "metadata": {},
   "source": [
    "K Nearest Neighbors: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24ce189",
   "metadata": {},
   "source": [
    "I chose k as 7 from tuning the parameters using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d36bcda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: spam, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_free:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "      <th>capital_run_length_average:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.778</td>\n",
       "      <td>3.756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.372</td>\n",
       "      <td>5.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.276</td>\n",
       "      <td>9.821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.137</td>\n",
       "      <td>3.537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.135</td>\n",
       "      <td>3.537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_free:  char_freq_!:  capital_run_length_average:\n",
       "0             0.32         0.778                        3.756\n",
       "1             0.14         0.372                        5.114\n",
       "2             0.06         0.276                        9.821\n",
       "3             0.31         0.137                        3.537\n",
       "4             0.31         0.135                        3.537"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining feature and target variables\n",
    "y = spam['spam']\n",
    "X = spam[['word_freq_free:', 'char_freq_!:', 'capital_run_length_average:']]\n",
    "print(y[0:5])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37c6332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ff69be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c1a6c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kneighborsclassifier__n_neighbors': 7}\n",
      "0.8540399652476107\n"
     ]
    }
   ],
   "source": [
    "knn_pipe = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "param_grid_knn = {'kneighborsclassifier__n_neighbors': range(1, 10)}\n",
    "grid_knn = GridSearchCV(knn_pipe, param_grid_knn)\n",
    "grid_knn.fit(X_train, y_train)\n",
    "print(grid_knn.best_params_)\n",
    "print(grid_knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "668e8a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on test data directly: 0.8523023457862728\n",
      "using k-fold cross validation: 0.8449275362318841\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train, y_train)\n",
    "print(f\"on test data directly: \" + str(knn.score(X_test, y_test)))\n",
    "print(f\"using k-fold cross validation: \" + str(mean(cross_val_score(knn, X_train, y_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3560e4c",
   "metadata": {},
   "source": [
    "9. Choose a second model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k).  Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae85cba2",
   "metadata": {},
   "source": [
    "Logistic Regression: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d93ef6",
   "metadata": {},
   "source": [
    "I chose the penalty as none because we are not using penalized logistic regression. I chose C to be 3 using GridSearchCV to tune the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dadc8d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:432: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  l2_reg_strength = 1.0 / C\n",
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/linear_model/_linear_loss.py:198: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  loss += 0.5 * l2_reg_strength * (weights @ weights)\n",
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/linear_model/_linear_loss.py:200: RuntimeWarning: invalid value encountered in multiply\n",
      "  grad[:n_features] = X.T @ grad_per_sample + l2_reg_strength * weights\n",
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:432: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  l2_reg_strength = 1.0 / C\n",
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/linear_model/_linear_loss.py:198: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  loss += 0.5 * l2_reg_strength * (weights @ weights)\n",
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/linear_model/_linear_loss.py:200: RuntimeWarning: invalid value encountered in multiply\n",
      "  grad[:n_features] = X.T @ grad_per_sample + l2_reg_strength * weights\n",
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:432: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  l2_reg_strength = 1.0 / C\n",
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/linear_model/_linear_loss.py:198: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  loss += 0.5 * l2_reg_strength * (weights @ weights)\n",
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/linear_model/_linear_loss.py:200: RuntimeWarning: invalid value encountered in multiply\n",
      "  grad[:n_features] = X.T @ grad_per_sample + l2_reg_strength * weights\n",
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:432: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  l2_reg_strength = 1.0 / C\n",
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/linear_model/_linear_loss.py:198: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  loss += 0.5 * l2_reg_strength * (weights @ weights)\n",
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/linear_model/_linear_loss.py:200: RuntimeWarning: invalid value encountered in multiply\n",
      "  grad[:n_features] = X.T @ grad_per_sample + l2_reg_strength * weights\n",
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:432: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  l2_reg_strength = 1.0 / C\n",
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/linear_model/_linear_loss.py:198: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  loss += 0.5 * l2_reg_strength * (weights @ weights)\n",
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/linear_model/_linear_loss.py:200: RuntimeWarning: invalid value encountered in multiply\n",
      "  grad[:n_features] = X.T @ grad_per_sample + l2_reg_strength * weights\n",
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logisticregression__C': 3}\n",
      "0.8097306689834927\n"
     ]
    }
   ],
   "source": [
    "logreg_pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "param_grid_logreg = {'logisticregression__C': np.arange(0,100)}\n",
    "grid_logreg = GridSearchCV(logreg_pipe, param_grid_logreg)\n",
    "grid_logreg.fit(X_train, y_train)\n",
    "print(grid_logreg.best_params_)\n",
    "print(grid_logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2df8a3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on test data directly: 0.8079930495221547\n",
      "using k-fold cross validation: 0.7939130434782609\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=3)\n",
    "logreg.fit(X_train, y_train)\n",
    "print(f\"on test data directly: \" + str(logreg.score(X_test, y_test)))\n",
    "print(f\"using k-fold cross validation: \" + str(mean(cross_val_score(logreg, X_train, y_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3337e5",
   "metadata": {},
   "source": [
    "The previous model, K Nearest Neighbors, predicts test data better than this model, Logistic Regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aed56fd",
   "metadata": {},
   "source": [
    "10. Choose a third model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f56cb93",
   "metadata": {},
   "source": [
    "Support Vector Classifier: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10dce2",
   "metadata": {},
   "source": [
    "I chose kernel C as 37 using GridSearchCV to tune parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14a36781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "5 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/pipeline.py\", line 382, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 251, in fit\n",
      "    fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 333, in _dense_fit\n",
      "    ) = libsvm.fit(\n",
      "  File \"sklearn/svm/_libsvm.pyx\", line 192, in sklearn.svm._libsvm.fit\n",
      "ValueError: C <= 0\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.8115942  0.81391304 0.81681159 0.81768116 0.82057971\n",
      " 0.82115942 0.82173913 0.82231884 0.82434783 0.82289855 0.82231884\n",
      " 0.82173913 0.8226087  0.82376812 0.82376812 0.82347826 0.82231884\n",
      " 0.82202899 0.82115942 0.82144928 0.82173913 0.82202899 0.82231884\n",
      " 0.82289855 0.8226087  0.82347826 0.82376812 0.82318841 0.82405797\n",
      " 0.82376812 0.82376812 0.82405797 0.82405797 0.82405797 0.82405797\n",
      " 0.82463768 0.82492754 0.82405797 0.82463768 0.82463768 0.82434783\n",
      " 0.82376812 0.82376812 0.82347826 0.82347826 0.82347826 0.82318841\n",
      " 0.82376812 0.82376812 0.82347826 0.82347826 0.82347826 0.82376812\n",
      " 0.82376812 0.82376812 0.82347826 0.82318841 0.82318841 0.82318841\n",
      " 0.82347826 0.82376812 0.82376812 0.82376812 0.82376812 0.82347826\n",
      " 0.82318841 0.82318841 0.82376812 0.82318841 0.82318841 0.82376812\n",
      " 0.82376812 0.82376812 0.82376812 0.82347826 0.82347826 0.82318841\n",
      " 0.82318841 0.82318841 0.82318841 0.82318841 0.82347826 0.82347826\n",
      " 0.82347826 0.82376812 0.82347826 0.82347826 0.82347826 0.82318841\n",
      " 0.82289855 0.82289855 0.82318841 0.82347826 0.82434783 0.82463768\n",
      " 0.82463768 0.82434783 0.82434783 0.82405797]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'svc__C': 37}\n",
      "0.8366637706342311\n"
     ]
    }
   ],
   "source": [
    "svc_pipe = make_pipeline(StandardScaler(), SVC())\n",
    "param_grid_svc = {'svc__C': np.arange(0,100)}\n",
    "grid_svc = GridSearchCV(svc_pipe, param_grid_svc)\n",
    "grid_svc.fit(X_train, y_train)\n",
    "print(grid_svc.best_params_)\n",
    "print(grid_svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67195c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on test data directly: 0.8210251954821894\n",
      "using k-fold cross validation: 0.803768115942029\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(C=37)\n",
    "svc.fit(X_train, y_train)\n",
    "print(f\"on test data directly: \" + str(svc.score(X_test, y_test)))\n",
    "print(f\"using k-fold cross validation: \" + str(mean(cross_val_score(svc, X_train, y_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6819fc3f",
   "metadata": {},
   "source": [
    "So far, the K Nearest Neighbors model remains to predict the best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674cf65",
   "metadata": {},
   "source": [
    "11. Choose a fourth model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcee49ff",
   "metadata": {},
   "source": [
    "Decision Trees:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ceb782",
   "metadata": {},
   "source": [
    "I chose max_depth to be 3 from tuning parameters using GridSearchCV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a93fb1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'decisiontreeclassifier__max_depth': 3}\n",
      "0.8531711555169418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "5 fits failed out of a total of 25.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/pipeline.py\", line 382, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 969, in fit\n",
      "    super().fit(\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/tree/_classes.py\", line 238, in fit\n",
      "    check_scalar(\n",
      "  File \"/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1480, in check_scalar\n",
      "    raise ValueError(\n",
      "ValueError: max_depth == 0, must be >= 1.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/usr/local/Cellar/jupyterlab/3.4.5/libexec/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.78811594 0.81188406 0.84376812 0.84289855]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tree_pipe = make_pipeline(StandardScaler(), DecisionTreeClassifier())\n",
    "param_grid_tree = {'decisiontreeclassifier__max_depth': np.arange(0,5)}\n",
    "grid_tree = GridSearchCV(tree_pipe, param_grid_tree)\n",
    "grid_tree.fit(X_train, y_train)\n",
    "print(grid_tree.best_params_)\n",
    "print(grid_tree.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d591828d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on test data directly: 0.8531711555169418\n",
      "using k-fold cross validation: 0.843768115942029\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=3)\n",
    "tree.fit(X_train, y_train)\n",
    "print(f\"on test data directly: \" + str(tree.score(X_test, y_test)))\n",
    "print(f\"using k-fold cross validation: \" + str(mean(cross_val_score(tree, X_train, y_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad6f309",
   "metadata": {},
   "source": [
    "So far, the best model to predict data is the Decision Tree model, with KNN being a close second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb1380",
   "metadata": {},
   "source": [
    "12. Now rerun your best model from questions 8 through 11, but this time add three new variables to the model that you think will increase prediction accuracy.   Did this model predict test data better than your previous models? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98ed841b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: spam, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_free:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "      <th>capital_run_length_average:</th>\n",
       "      <th>word_freq_credit:</th>\n",
       "      <th>word_freq_money:</th>\n",
       "      <th>char_freq_$:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.778</td>\n",
       "      <td>3.756</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.372</td>\n",
       "      <td>5.114</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.276</td>\n",
       "      <td>9.821</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.137</td>\n",
       "      <td>3.537</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.135</td>\n",
       "      <td>3.537</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_free:  char_freq_!:  capital_run_length_average:  \\\n",
       "0             0.32         0.778                        3.756   \n",
       "1             0.14         0.372                        5.114   \n",
       "2             0.06         0.276                        9.821   \n",
       "3             0.31         0.137                        3.537   \n",
       "4             0.31         0.135                        3.537   \n",
       "\n",
       "   word_freq_credit:  word_freq_money:  char_freq_$:  \n",
       "0               0.00              0.00         0.000  \n",
       "1               0.00              0.43         0.180  \n",
       "2               0.32              0.06         0.184  \n",
       "3               0.00              0.00         0.000  \n",
       "4               0.00              0.00         0.000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = spam['spam']\n",
    "X1 = spam[['word_freq_free:', 'char_freq_!:', 'capital_run_length_average:', 'word_freq_credit:', 'word_freq_money:', 'char_freq_$:']]\n",
    "print(y1[0:5])\n",
    "X1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d231ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X1)\n",
    "X1_scaled = scaler.transform(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83428bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1_scaled, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdbdbc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on test data directly: 0.8618592528236316\n",
      "using k-fold cross validation: 0.851304347826087\n"
     ]
    }
   ],
   "source": [
    "tree1 = DecisionTreeClassifier(max_depth=3)\n",
    "tree1.fit(X1_train, y1_train)\n",
    "print(f\"on test data directly: \" + str(tree1.score(X1_test, y1_test)))\n",
    "print(f\"using k-fold cross validation: \" + str(mean(cross_val_score(tree1, X1_train, y1_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031ed0a7",
   "metadata": {},
   "source": [
    "This model predicted test data better than my previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f296a",
   "metadata": {},
   "source": [
    "13. Rerun all your other models with this final set of six variables, evaluate prediction error, and choose a final model.  Why did you select this model among all of the models that you ran? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27b035fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on test data directly: 0.8618592528236316\n",
      "using k-fold cross validation: 0.8785507246376811\n"
     ]
    }
   ],
   "source": [
    "knn1 = KNeighborsClassifier(n_neighbors=7)\n",
    "knn1.fit(X1_train, y1_train)\n",
    "print(f\"on test data directly: \" + str(knn1.score(X1_test, y1_test)))\n",
    "print(f\"using k-fold cross validation: \" + str(mean(cross_val_score(knn1, X1_train, y1_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05ad3db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on test data directly: 0.8688097306689835\n",
      "using k-fold cross validation: 0.876231884057971\n"
     ]
    }
   ],
   "source": [
    "svc1 = SVC(C=37)\n",
    "svc1.fit(X1_train, y1_train)\n",
    "print(f\"on test data directly: \" + str(svc1.score(X1_test, y1_test)))\n",
    "print(f\"using k-fold cross validation: \" + str(mean(cross_val_score(svc1, X1_train, y1_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c37267aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on test data directly: 0.8462206776715899\n",
      "using k-fold cross validation: 0.8489855072463768\n"
     ]
    }
   ],
   "source": [
    "logreg1 = LogisticRegression(C=3)\n",
    "logreg1.fit(X1_train, y1_train)\n",
    "print(f\"on test data directly: \" + str(logreg1.score(X1_test, y1_test)))\n",
    "print(f\"using k-fold cross validation: \" + str(mean(cross_val_score(logreg1, X1_train, y1_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4161df",
   "metadata": {},
   "source": [
    "My final model is the K Nearest Neighbors with the 6 variable features as predictors. I selected this model because it had the highest score and cross validation score overall. By using the highest cross validation score to assess, we can assume this model stands better against overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf88b233",
   "metadata": {},
   "source": [
    "14. What variable that currently is not in your model, if included, would be likely to increase your final model's predictive power?  For this answer try to speculate about a variable outside the variables available in the data that would improve you model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4175b48b",
   "metadata": {},
   "source": [
    "A variable that is not currently in my model that would likely increase predictive power is 'char_freq_emoji' which detects whether or not the email contains any character from the emoji board. This would likely increase predictive power as many spam emails, such as ones from retail websites and subscriptions, contain emojis and professional emails should not use emojis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd39fe4",
   "metadata": {},
   "source": [
    "15. Lastly, you have listed each of the models that we have learned to use to predict dependent variables like spam.  List each model we have focused on in class thus far that you could use to evaluate data with a continuous dependent variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f5c93b",
   "metadata": {},
   "source": [
    "Linear Regression, Lasso Regression, Ridge Regression, K Nearest Regressors, and Decision Tree Regressor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2def08fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
